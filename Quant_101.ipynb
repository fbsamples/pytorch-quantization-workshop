{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>(Fake) Quantization from scratch</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from copy import deepcopy\n",
    "import requests\n",
    "from PIL import Image\n",
    "import ast\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions to \n",
    "    \n",
    "    a) load an image \n",
    "    b) preprocess for inference\n",
    "    c) get top predicted label from model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(url_or_path):\n",
    "    if url_or_path.startswith(\"https\"):\n",
    "        img = Image.open(requests.get(url_or_path, stream=True).raw)\n",
    "    else:\n",
    "        img = Image.open(url_or_path)\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess_image(img):\n",
    "    IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)\n",
    "        ])\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    return img\n",
    "\n",
    "\n",
    "imgnet_idx_to_label = requests.get(\"https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt\")\n",
    "imgnet_idx_to_label = ast.literal_eval(imgnet_idx_to_label.text)\n",
    "\n",
    "def logits_to_label(outp):\n",
    "    outp = F.softmax(outp, dim=1)\n",
    "    score, idx = torch.topk(outp, 1)\n",
    "    idx.squeeze_()\n",
    "    predicted_label = imgnet_idx_to_label[idx.item()]\n",
    "    return predicted_label, score.squeeze().item()\n",
    "\n",
    "\n",
    "def get_predictions(model, img_path):\n",
    "    img = load_image(img_path)\n",
    "    img = preprocess_image(img)\n",
    "    logits = model(img)\n",
    "    label, score = logits_to_label(logits)\n",
    "    print(label, '(', score, ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a PyTorch pretrained Resnet model\n",
    "\n",
    "Resnets are image classification models.\n",
    "\n",
    "PyTorch offers Resnets trained on the Imagenet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# Set the model up for inference\n",
    "resnet.eval()\n",
    "resnet.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick test that the model inference works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on an image of a wolf\n",
    "wolf_img_url = \"https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png\"\n",
    "get_predictions(resnet, wolf_img_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model perf profiling\n",
    "\n",
    "With quantization we are making the model faster and smaller. Let's write some functions to evaluate the model on size and latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile(model, input):\n",
    "    print_size_of_model(model)\n",
    "    module_latency(model, input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to print the size of the model. \n",
    "\n",
    "HINT: `os.path.getsize()` returns the size of a file in bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model):\n",
    "    \n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to see the solution\n",
    "!wget https://raw.githubusercontent.com/suraj813/da-content/main/quantization_workshop/code/101/sizeof.py\n",
    "%pycat sizeof.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to measure model inference time.\n",
    "\n",
    "HINT: Use `torch.inference_mode` when running a model purely for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def module_latency(model, input, num_tests=10):\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "\n",
    "        # TODO\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to see the solution\n",
    "!wget https://raw.githubusercontent.com/suraj813/da-content/main/quantization_workshop/code/101/latency.py\n",
    "%pycat latency.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profile the Resnet18 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# 1. load an image\n",
    "# 2. preprocess image\n",
    "# 3. profile resnet inference on the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to see the solution\n",
    "!wget https://raw.githubusercontent.com/suraj813/qnt_workshop/master/static/code/101/prof.py\n",
    "%pycat prof.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we're going to quantize only the last layer of the resnet.\n",
    "\n",
    "The last layer is a linear module. It is also known as the classifier because it computes the probability of an image's label from its features.\n",
    "\n",
    "First let's separate the classifier from the rest of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the resnet model layers\n",
    "print(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the classifier \n",
    "fp32_fc = deepcopy(resnet.fc)\n",
    "\n",
    "# \"Remove\" the classifier from resnet by replacing it with a no-op module.\n",
    "resnet.fc = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure everything still works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(resnet, fp32_fc)\n",
    "get_predictions(model, wolf_img_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Attempt 1: Use `round` as mapping function\n",
    "\n",
    "Quantization mapping functions also include naive functions like `round`. \n",
    "\n",
    "Make a copy of the FP32 classifier and round its weight and bias tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_fc = deepcopy(fp32_fc)\n",
    "rounded_fc.weight = torch.nn.Parameter(torch.round(rounded_fc.weight), requires_grad=False)\n",
    "rounded_fc.bias = torch.nn.Parameter(torch.round(rounded_fc.bias), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sounds too good to be true?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(resnet, rounded_fc)\n",
    "get_predictions(model, wolf_img_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already knew [this wouldn't work](https://en.wikipedia.org/wiki/There_ain%27t_no_such_thing_as_a_free_lunch), but it's good to clarify exactly why.\n",
    "\n",
    "Let's see the what the weights' values are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "_, _, _ = plt.hist(fp32_fc.weight.detach().flatten(), density=True, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO\n",
    "\n",
    "The reason this failed is because..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to see the solution\n",
    "!wget https://raw.githubusercontent.com/suraj813/qnt_workshop/master/static/code/101/roundfail.txt\n",
    "%pycat roundfail.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Attempt 2: Scale + Round as mapping function\n",
    "\n",
    "This time, we rescale the parameters into an appropriate output range before rounding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output range\n",
    "\n",
    "* The output range defines the min and max values in the quantized space.\n",
    "* The range depends on the quantization precision. \n",
    "\n",
    "HINT: The range of an 8-bit number is [-2^7, 2^7 - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_range(bits):\n",
    "    # TODO\n",
    "\n",
    "print(\"For 16-bit quantization, the quantized range is \", get_output_range(16))\n",
    "print(\"For 8-bit quantization, the quantized range is \", get_output_range(8))\n",
    "print(\"For 3-bit quantization, the quantized range is \", get_output_range(3))\n",
    "print(\"For 2-bit quantization, the quantized range is \", get_output_range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to see the solution\n",
    "!wget https://raw.githubusercontent.com/suraj813/qnt_workshop/master/static/code/101/output_range.py\n",
    "%pycat output_range.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we're going to use 8-bit quantization. So the output range to scale our parameters is [-128, 127]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving from FP32 to INT8\n",
    "\n",
    "<img src=\"./img/scaling.png\" width=\"600\" />\n",
    "\n",
    "Generally speaking, what we're doing here is an affine transformation from 32-bit space to 8-bit space.\n",
    "\n",
    "These are of the form `y  = Ax + B`\n",
    "\n",
    "The two parameters for this transformation are: \n",
    "* The scaling factor `S`     \n",
    "* The zero-point `Z`         \n",
    "\n",
    "So our transformation looks like `Q(x) = round(x/S + Z)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantization_params(input_range, output_range):\n",
    "    # TODO\n",
    "    return S, Z\n",
    "\n",
    "\n",
    "def quantize(x, S, Z):\n",
    "    # TODO\n",
    "    return x_q\n",
    "\n",
    "\n",
    "def dequantize(x_q, S, Z):\n",
    "    # TODO\n",
    "    return x\n",
    "\n",
    "\n",
    "def quantize_int8(x):\n",
    "    input_range = x.min(), x.max()\n",
    "    output_range = get_output_range(8)\n",
    "    S, Z = get_quantization_params(input_range, output_range)\n",
    "    x_q = quantize(x, S, Z)\n",
    "    return x_q, S, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to see the solution\n",
    "!wget https://raw.githubusercontent.com/suraj813/qnt_workshop/master/static/code/101/qparams.py\n",
    "%pycat qparams.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the functions we need to quantize our classifier.\n",
    "\n",
    "Like before, we quantize each parameter in the layer (`weights` and `bias` in this case). \n",
    "\n",
    "We will also quantize the incoming features to the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_inputs(img):\n",
    "    features = resnet(img)\n",
    "    X_q, S_x, Z_x = quantize_int8(features)\n",
    "    return (X_q, S_x, Z_x)\n",
    "\n",
    "\n",
    "def quantize_classifier(clf):\n",
    "    W_q, S_w, Z_w = quantize_int8(clf.weight)\n",
    "    b_q, S_b, Z_b = quantize_int8(clf.bias)\n",
    "    return (W_q, S_w, Z_w), (b_q, S_b, Z_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, quantized operators run in specialized backends like FBGEMM and QNNPACK.\n",
    "\n",
    "FBGEMM is an open source linear algebra library from Meta for reduced-precision DL inference.\n",
    "\n",
    "We can simulate the INT8 Linear module by first dequantizing everything back to FP32, and then running the multiply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int8_linear_sim(quantized_input, quantized_weights, quantized_bias):\n",
    "    X = dequantize(*quantized_input)\n",
    "    W = dequantize(*quantized_weights)\n",
    "    b = dequantize(*quantized_bias)\n",
    "    return b + X @ W.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to run our \"fake-quantized\" classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still in FP32 space...\n",
    "img = preprocess_image(load_image(wolf_img_url))\n",
    "features = resnet(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we move to INT8\n",
    "\n",
    "inputs_q = quantize_inputs(features)\n",
    "weights_q, bias_q = quantize_classifier(fp32_fc)\n",
    "logits_q =  int8_linear_sim(inputs_q, weights_q, bias_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Compare this with FP32 logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = fp32_fc(features)\n",
    "\n",
    "\n",
    "print(\"Non-Quantized output:\\n\", logits[:, :10], \"\\n\")\n",
    "print(\"Quantized output:\\n\", logits_q[:, :10], \"\\n\")\n",
    "\n",
    "quantization_error = (logits_q - logits).mean()\n",
    "print(\"Quantization error = \", quantization_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantization error is pretty sizable at 1e-3. \n",
    "\n",
    "Eyeballing the outputs, the logits from the quantized and non-quantized layers seem fairly different too.\n",
    "\n",
    "Let's see by how much are the quantized predictions off..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-quantized predictions\n",
    "logits_to_label(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantized predictions\n",
    "\n",
    "logits_to_label(logits_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try more images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = \"./img/swan-3299528_1280.jpeg\"\n",
    "# img_url = \"https://static.scientificamerican.com/sciam/cache/file/32665E6F-8D90-4567-9769D59E11DB7F26_source.jpg\"\n",
    "# img_url = \"https://media.newyorker.com/photos/5dfab39dde5fcf00086aec77/1:1/w_1706,h_1706,c_limit/Lane-Cats.jpg\"\n",
    "\n",
    "\n",
    "# FP32\n",
    "img = preprocess_image(load_image(img_url))\n",
    "features = resnet(img)\n",
    "\n",
    "logits = fp32_fc(features)\n",
    "\n",
    "# INT8\n",
    "inputs_q = quantize_inputs(features)\n",
    "logits_q =  int8_linear_sim(inputs_q, weights_q, bias_q)\n",
    "\n",
    "\n",
    "# Compare predictions\n",
    "print(\"Non-Quantized prediction:\")\n",
    "logits_to_label(logits)\n",
    "print()\n",
    "print(\"Quantized prediction:\")\n",
    "logits_to_label(logits_q)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b2c14c5f2a3b21e6c2412c8196f5145870350e81c0b737cae3e5c60eb1e1eac"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch_p38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
