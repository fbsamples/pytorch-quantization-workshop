{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from copy import deepcopy\n",
    "import requests\n",
    "from PIL import Image\n",
    "import ast\n",
    "\n",
    "cls_idx = requests.get(\"https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt\")\n",
    "cls_idx = ast.literal_eval(cls_idx.text)\n",
    "\n",
    "\n",
    "def load_img(url):\n",
    "    IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)\n",
    "        ])\n",
    "    if url.startswith(\"https\"):\n",
    "        img = Image.open(requests.get(url, stream=True).raw)\n",
    "    else:\n",
    "        img = Image.open(url)\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_predictions(outp):\n",
    "    outp = F.softmax(outp, dim=1)\n",
    "    score, idx = torch.topk(outp, 1)\n",
    "    idx.squeeze_()\n",
    "    predicted_label = cls_idx[idx.item()]\n",
    "    print(predicted_label, '(', score.squeeze().item(), ')')\n",
    "\n",
    "\n",
    "def print_sizeof(model):\n",
    "    total = 0\n",
    "    for p in model.parameters():\n",
    "        total += p.numel() * p.element_size()\n",
    "    total /= 1e6\n",
    "    print(\"Model size: \", total, \" MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals of Quantization\n",
    "* Quantization is the process of reducing the size of data. \n",
    "* It uses a `mapping function` to convert values in floating-point space to integer space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# floor, ceil and round are also quantization mapping functions\n",
    "\n",
    "import math\n",
    "\n",
    "print(math.floor(3.14159265359))\n",
    "print(math.ceil(3.14159265359))\n",
    "print(round(3.14159265359))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While its roots are in digital signal processing (for digital encoding and lossy compression), quantization techniques are also used to reduce the size of deep neural networks (DNNs).\n",
    "\n",
    "DNN parameters are typically 32-bit floating point numbers; using quantization, we can represent them as 8-bit (or lower) integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization of neural networks from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop, we'll \n",
    "\n",
    "a) Load a pretrained Resnet model\n",
    "\n",
    "b) Quantize the last layer (classifier) from scratch\n",
    "\n",
    "c) Compare accuracy performance with non-quantized classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "resnet = models.resnet18(pretrained=True).eval()\n",
    "resnet.requires_grad_(False)\n",
    "\n",
    "# Extract the classifier before removing from resnet\n",
    "fp32_fc = deepcopy(resnet.fc)\n",
    "\n",
    "# Remove classifier from resnet model. This is now a Resnet feature extractor.\n",
    "resnet.fc = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing that we didn't screw anything up..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timber wolf, grey wolf, gray wolf, Canis lupus ( 0.44803616404533386 )\n"
     ]
    }
   ],
   "source": [
    "wolf_img = \"https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png\"\n",
    "img = load_img(wolf_img)\n",
    "\n",
    "model = torch.nn.Sequential(resnet, fp32_fc)\n",
    "logits = model(img)\n",
    "get_predictions(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1: Round\n",
    "Quantization mapping functions also include naive functions like `round`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_fc = deepcopy(fp32_fc)\n",
    "rounded_fc.weight = torch.nn.Parameter(torch.round(rounded_fc.weight), requires_grad=False)\n",
    "rounded_fc.bias = torch.nn.Parameter(torch.round(rounded_fc.bias), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sounds too good to be true?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rhinoceros beetle ( 0.01966511830687523 )\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Sequential(resnet, rounded_fc)\n",
    "logits = model(img)\n",
    "get_predictions(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already knew [this wouldn't work](https://en.wikipedia.org/wiki/There_ain%27t_no_such_thing_as_a_free_lunch), but it's good to get it out of the way.\n",
    "\n",
    "The reason this failed is because our classifier's parameters are between [-0.2, 0.4]. By directly rounding these, we just zeroed out our layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "_, _, _ = plt.hist(fp32_fc.weight.detach().flatten(), density=True, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2: Scale before Round\n",
    "\n",
    "This time, we rescale the parameters into an appropriate output range before rounding. \n",
    "\n",
    "What's a good output range? It depends on the quantization precision you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the quantized ouput range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 16-bit quantization, the quantized range is  (-32768, 32767)\n",
      "For 8-bit quantization, the quantized range is  (-128, 127)\n",
      "For 4-bit quantization, the quantized range is  (-8, 7)\n"
     ]
    }
   ],
   "source": [
    "def get_output_range(bits):\n",
    "    alpha_q = -2 ** (bits - 1)\n",
    "    beta_q = 2 ** (bits - 1) - 1\n",
    "    return alpha_q, beta_q\n",
    "\n",
    "\n",
    "print(\"For 16-bit quantization, the quantized range is \", get_output_range(16))\n",
    "print(\"For 8-bit quantization, the quantized range is \", get_output_range(8))\n",
    "print(\"For 4-bit quantization, the quantized range is \", get_output_range(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we're going to use 8-bit quantization. So the output range to scale our parameters is [-128, 127]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving from FP32 to INT8\n",
    "\n",
    "<img src=\"img/scaling.png\" width=\"600\">\n",
    "\n",
    "Generally speaking, what we're doing here is an affine transformation from 32-bit space to 8-bit space.\n",
    "\n",
    "These are of the form `y  = Ax + B`\n",
    "\n",
    "The two parameters for this transformation are: \n",
    "* The scaling factor `S`     \n",
    "* The zero-point `Z`         \n",
    "\n",
    "So our transformation looks like `Q(x) = round(x/S + Z)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantization_params(input_range, output_range):\n",
    "    min_val, max_val = input_range\n",
    "    alpha_q, beta_q = output_range\n",
    "    S = (max_val - min_val) / (beta_q - alpha_q)\n",
    "    Z = alpha_q - (min_val / S)\n",
    "    return S, Z\n",
    "\n",
    "\n",
    "def scale_transform(x, S, Z):\n",
    "    x_q = 1/S * x + Z  \n",
    "    x_q = torch.round(x_q).to(torch.int8)\n",
    "    return x_q\n",
    "\n",
    "\n",
    "def quantize_int8(x):\n",
    "    S, Z = get_quantization_params(input_range=(x.min(), x.max(),), output_range=(-128, 127))\n",
    "    x_q = scale_transform(x, S, Z)\n",
    "    return x_q, S, Z\n",
    "\n",
    "\n",
    "def dequantize(x_q, S, Z):\n",
    "    x = S * (x_q - Z)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the functions we need to quantize our classifier.\n",
    "\n",
    "Like before, we quantize each parameter in the layer (`weights` and `bias` in this case). \n",
    "\n",
    "We will also quantize the inputs to the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantize classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_classifier(clf):\n",
    "    W_q, S_w, Z_w = quantize_int8(clf.weight)\n",
    "    b_q, S_b, Z_b = quantize_int8(clf.bias)\n",
    "    return (W_q, S_w, Z_w, b_q, S_b, Z_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantize inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_inputs(img):\n",
    "    features = resnet(img)\n",
    "    X_q, S_x, Z_x = quantize_int8(features)\n",
    "    return (X_q, S_x, Z_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantized Matrix Multiplication\n",
    "\n",
    "In PyTorch, the quantized operators run in specialized backends like FBGEMM and QNNPACK.\n",
    "\n",
    "We can simulate the INT8 matmul by first dequantizing everything to FP32 and then running the multiply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int8_matmul_sim(quantized_input, quantized_layer):\n",
    "    X = dequantize(*quantized_input)\n",
    "    W = dequantize(*quantized_layer[:3])\n",
    "    b = dequantize(*quantized_layer[3:])\n",
    "    return b + X @ W.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Quantized and Non-Quantized forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Quantized\n",
    "model = torch.nn.Sequential(resnet, fp32_fc)\n",
    "logits = model(img)\n",
    "\n",
    "# Quantized\n",
    "inputs_q = quantize_inputs(img)\n",
    "classifier_q = quantize_classifier(fp32_fc)\n",
    "logits_q = int8_matmul_sim(inputs_q, classifier_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Q and N-Q logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Quantized output:\n",
      " tensor([[ 0.2827, -1.5461,  1.2094, -0.2907, -3.6378,  0.8214, -1.3164, -3.7967,\n",
      "         -1.8691, -1.7165]]) \n",
      "\n",
      "Quantized output:\n",
      " tensor([[ 0.2997, -1.5684,  1.2295, -0.2559, -3.6580,  0.8425, -1.3213, -3.8168,\n",
      "         -1.8718, -1.7032]]) \n",
      "\n",
      "Quantization error =  tensor(-0.0010)\n"
     ]
    }
   ],
   "source": [
    "# Compare quantized and non-quantized logits\n",
    "print(\"Non-Quantized output:\\n\", logits[:, :10], \"\\n\")\n",
    "print(\"Quantized output:\\n\", logits_q[:, :10], \"\\n\")\n",
    "\n",
    "quantization_error = (logits_q - logits).mean()\n",
    "print(\"Quantization error = \", quantization_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantization error is pretty sizable at 1e-3. \n",
    "\n",
    "Eyeballing the outputs, the logits from the quantized and non-quantized layers seem fairly different too.\n",
    "\n",
    "Let's see by how much are the quantized predictions off..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Quantized prediction:\n",
      "timber wolf, grey wolf, gray wolf, Canis lupus ( 0.44803616404533386 )\n",
      "\n",
      "Quantized prediction:\n",
      "timber wolf, grey wolf, gray wolf, Canis lupus ( 0.445095956325531 )\n"
     ]
    }
   ],
   "source": [
    "# check their outputs for same input\n",
    "print(\"Non-Quantized prediction:\")\n",
    "get_predictions(logits)\n",
    "print()\n",
    "print(\"Quantized prediction:\")\n",
    "get_predictions(logits_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not by much! The quantized logits predict the same class, albeit with slightly lower confidence.\n",
    "\n",
    "Let's try more images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Quantized prediction:\n",
      "goose ( 0.5297383666038513 )\n",
      "\n",
      "Quantized prediction:\n",
      "goose ( 0.5486957430839539 )\n"
     ]
    }
   ],
   "source": [
    "# Similarly for an image of a swan\n",
    "img_url = \"img/swan-3299528_1280.jpeg\"\n",
    "# img_url = \"https://static.scientificamerican.com/sciam/cache/file/32665E6F-8D90-4567-9769D59E11DB7F26_source.jpg\"\n",
    "# img_url = \"https://media.newyorker.com/photos/5dfab39dde5fcf00086aec77/1:1/w_1706,h_1706,c_limit/Lane-Cats.jpg\"\n",
    "\n",
    "img = load_img(img_url)\n",
    "\n",
    "# Non-Quantized\n",
    "model = torch.nn.Sequential(resnet, fp32_fc)\n",
    "logits = model(img)\n",
    "\n",
    "# Quantized\n",
    "inputs_q = quantize_inputs(img)\n",
    "classifier_q = quantize_classifier(fp32_fc)\n",
    "logits_q = int8_matmul_sim(inputs_q, classifier_q)\n",
    "\n",
    "\n",
    "# Compare predictions\n",
    "print(\"Non-Quantized prediction:\")\n",
    "get_predictions(logits)\n",
    "print()\n",
    "print(\"Quantized prediction:\")\n",
    "get_predictions(logits_q)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
