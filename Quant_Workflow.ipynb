{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms, datasets\n",
    "from copy import deepcopy\n",
    "import requests\n",
    "from PIL import Image\n",
    "from resnet_cifar import Trainer, cifar_dataloader\n",
    "\n",
    "\n",
    "def load_img(url):\n",
    "    IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)\n",
    "        ])\n",
    "    if url.startswith(\"https\"):\n",
    "        img = Image.open(requests.get(url, stream=True).raw)\n",
    "    else:\n",
    "        img = Image.open(url)\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_predictions(outp):\n",
    "    cls_idx = {\n",
    "        0: 'airplane',\n",
    "        1: 'automobile',\n",
    "        2: 'bird',\n",
    "        3: 'cat',\n",
    "        4: 'deer',\n",
    "        5: 'dog',\n",
    "        6: 'frog',\n",
    "        7: 'horse',\n",
    "        8: 'ship',\n",
    "        9: 'truck'}\n",
    "    outp = F.softmax(outp, dim=1)\n",
    "    score, idx = torch.topk(outp, 1)\n",
    "    idx.squeeze_()\n",
    "    predicted_label = cls_idx[idx.item()]\n",
    "    print(predicted_label, '(', score.squeeze().item(), ')')\n",
    "\n",
    "\n",
    "def print_sizeof(model):\n",
    "    total = 0\n",
    "    for p in model.parameters():\n",
    "        total += p.numel() * p.element_size()\n",
    "    total /= 1e6\n",
    "    print(\"Model size: \", total, \" MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flowchart for using Quantization in PyTorch\n",
    "\n",
    "<img src=\"./img/quantization-flowchart.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10M+ Parameters?\n",
    "\n",
    "<img src=\"./img/flowchart-check1.png\" width=\"300\" />\n",
    "\n",
    "Quantization works best on models with 10M+ parameters. [[1](https://arxiv.org/pdf/1806.08342.pdf)]\n",
    "\n",
    "Large models are more robust to quantization error. Overparameterized models generally have more degrees of freedom and can afford the precision drops with quantization.\n",
    "\n",
    "As with most thumb rules, YMMV. Quantization is an active area of research, and this might become more permissive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_large_enough(model):\n",
    "    n_params = sum([p.numel() for p in model.parameters()])\n",
    "    return n_params > 1e7, n_params // 1e6\n",
    "\n",
    "print(\"resnet18: \", is_large_enough(models.resnet18()))\n",
    "print(\"resnet50: \", is_large_enough(models.resnet50()))\n",
    "print(\"mobilenet_large: \", is_large_enough(models.mobilenet_v3_large()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP32-pretrained checkpoint?\n",
    "\n",
    "<img src=\"./img/flowchart-check2.png\" width=\"300\" />\n",
    "\n",
    "Quantized inference works best on models that were originally trained in FP32 (like all pretrained models in PyTorch (vision, audio and text)).\n",
    "\n",
    "This allows the model to learn many fine-grained parameters, that can later be quantized for inference.\n",
    "\n",
    "Even Quantization-Aware Training (more on this below) uses FP32 arithmetic to train the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from checkpoint \n",
    "\n",
    "# weights = torch.hub.load_state_dict_from_url(\"https://quantization-workshop.s3.amazonaws.com/resnet50_cifar_weights.pth\", map_location=\"cpu\")\n",
    "# resnet = models.resnet50(pretrained=False, num_classes=10)\n",
    "\n",
    "weights = torch.hub.load_state_dict_from_url(\"https://quantization-workshop.s3.amazonaws.com/resnet18_cifar_weights.pth\", map_location=\"cpu\")\n",
    "resnet = models.resnet18(pretrained=False, num_classes=10)\n",
    "resnet.load_state_dict(weights)\n",
    "\n",
    "param_0 = next(iter(resnet.parameters()))\n",
    "print(\"Model precision: \", param_0.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a supported backend?\n",
    "\n",
    "<img src=\"./img/flowchart-check3.png\" width=\"300\" />\n",
    "\n",
    "Backend refers to the hardware-specific kernels that support quantization. This controls the numerics engine that does the integer arithmetic.\n",
    "\n",
    "`torch.backends.quantized.engine` specifies the backend to be used.\n",
    "\n",
    "Using an incorrect backend engine for your hardware will result in (much) slower inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "chip = platform.processor()\n",
    "\n",
    "if chip == 'arm':\n",
    "    backend = 'qnnpack'\n",
    "elif chip in ['x86_64', 'i386']:\n",
    "    backend = 'fbgemm'\n",
    "else:\n",
    "    raise SystemError(\"Backend is not supported\")\n",
    "\n",
    "print(f\"Using {backend} backend engine for {chip} CPU\")\n",
    "\n",
    "torch.backends.quantized.engine = backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile FP32 model inference\n",
    "\n",
    "<img src=\"./img/flowchart-check4.png\" width=\"300\" />\n",
    "\n",
    "Let's establish a baseline for model size, inference latency and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "    \n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "def profile(model):\n",
    "    print_size_of_model(model)\n",
    "    print(\"=\"*20)\n",
    "    Trainer(model, -1).evaluate(max_batch=30)  # latency + accuracy on CIFAR test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resnet FP32 profile:\")\n",
    "profile(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the predominant layers in the model?\n",
    "\n",
    "<img src=\"./img/flowchart-check5.png\" width=\"300\" />\n",
    "\n",
    "While dynamic quantization has more overhead than static quantization, some operators (like recurrent layers) aren't supported by static quantization. (See [Operator coverage](https://pytorch.org/docs/stable/quantization.html#:~:text=these%20quantization%20types.-,Operator%20coverage,-varies%20between%20dynamic)).\n",
    "\n",
    "Knowing which layers are in our model can inform our quantization strategy.\n",
    "\n",
    "\n",
    "#### Thumb rule\n",
    "\n",
    "* For recurrent and transformer layers, use Dynamic quantization.\n",
    "* For linear layers, you can use either Dynamic or Static quantization.\n",
    "* For everything else, use Static quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_quant_strategy(model):\n",
    "    from collections import Counter\n",
    "    layer_counts = Counter([type(x).__name__ for x in model.modules()])\n",
    "    print(\"Model consists of: \", layer_counts)\n",
    "    \n",
    "    dyn = [0, 0]\n",
    "    stat = [0, 0]\n",
    "\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, 'weight'):    \n",
    "            name = type(m).__name__\n",
    "            params = m.weight.numel()\n",
    "            if name in ['RNN', 'LSTM', 'GRU', 'LSTMCell', 'RNNCell', 'GRUCell', 'Linear']:\n",
    "                dyn[0] += 1\n",
    "                dyn[1] += params\n",
    "            if 'Conv' in name or name == 'Linear':\n",
    "                stat[0] += 1\n",
    "                stat[1] += params\n",
    "    print()\n",
    "    print(\"Dynamic quantization\")\n",
    "    print(\"====================\")\n",
    "    print(f\"Layers: {dyn[0]} || Parameters: {format(dyn[1], 'g')}\")\n",
    "    print()\n",
    "    print(\"Static quantization\")\n",
    "    print(\"====================\")\n",
    "    print(f\"Layers: {stat[0]} || Parameters: {format(stat[1], 'g')}\")\n",
    "    \n",
    "\n",
    "optimal_quant_strategy(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Dynamic Quantization\n",
    "\n",
    "<img src=\"./img/flowchart-check6.png\" width=\"300\" />\n",
    "\n",
    "[Dynamic Quantization API](https://pytorch.org/docs/stable/generated/torch.quantization.quantize_dynamic.html?highlight=quantize_dynamic#torch.quantization.quantize_dynamic)\n",
    "\n",
    "[Dynamic Quantization Tutorial](https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "\n",
    "dynamic_qconfig = torch.quantization.default_dynamic_qconfig\n",
    "qconfig_dict = {\n",
    "    # Global Config\n",
    "    \"\": dynamic_qconfig\n",
    "}\n",
    "\n",
    "model_prepared = prepare_fx(resnet, qconfig_dict)\n",
    "dynamic_resnet = convert_fx(model_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance of dynamic-quantized Resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resnet Dynamic-Quant Profile:\")\n",
    "profile(dynamic_resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Static Quantization\n",
    "\n",
    "<img src=\"./img/flowchart-check7_1.png\" width=\"300\" />\n",
    "<br>\n",
    "<img src=\"./img/flowchart-check7_2.png\" width=\"300\" />\n",
    "\n",
    "Static quantization has faster inference than dynamic quantization because it eliminates the float<->int conversion costs between layers\n",
    "\n",
    "### Manual approach - using Eager Mode\n",
    "\n",
    "Explicitly perform the following steps:\n",
    "\n",
    "<img src=\"./img/ptq-flowchart.png\" width=\"300\" />\n",
    "\n",
    "* Manually identify sequence of fusable modules\n",
    "* Manually insert stubs to quantize and dequantize activations\n",
    "* Functional ops (eg: `torch.nn.functional.linear`) aren't supported\n",
    "\n",
    "[Module Fusion Tutorial](https://pytorch.org/tutorials/recipes/fuse.html)\n",
    "\n",
    "[Static Quantization (Eager Mode) Tutorial](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easier approach - using FX Graph Mode\n",
    "\n",
    "<img src=\"./img/ptq-fx-flowchart.png\" width=\"300\" />\n",
    "\n",
    "* Just 2 function calls: `prepare_fx` and `convert_fx`\n",
    "* Automates all the above steps under the hood using `torch.fx`\n",
    "\n",
    "[`prepare_fx` API](https://pytorch.org/docs/stable/generated/torch.quantization.quantize_fx.prepare_fx.html#torch.quantization.quantize_fx.prepare_fx)\n",
    "\n",
    "[`convert_fx` API](https://pytorch.org/docs/stable/generated/torch.quantization.quantize_fx.convert_fx.html#torch.quantization.quantize_fx.convert_fx)\n",
    "\n",
    "[Static Quantization (FX Graph Mode) Tutorial](https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QConfig\n",
    "\n",
    "In FX Quantization, the `qconfig_dict` offers fine-grained control of the model's quantization process.\n",
    "Setting a `qconfig=None` skips quantization for that module.\n",
    "\n",
    "```python\n",
    "qconfig_dict = {\n",
    "    # Global Config\n",
    "    \"\": qconfig,\n",
    "\n",
    "    # Module-specific config (by class)\n",
    "    \"object_type\": [\n",
    "        (torch.nn.Conv2d, qconfig),\n",
    "        (torch.nn.functional.add, None),  # skips quantization for this module\n",
    "        ...,\n",
    "        ],\n",
    "    \n",
    "    # Module-specific config (by name)\n",
    "    \"module_name\": [\n",
    "        (\"foo.bar\", qconfig)\n",
    "        ...,\n",
    "    ],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "qconfig_dict = {\n",
    "    # Global Config\n",
    "    \"\": static_qconfig,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "\n",
    "\n",
    "def static_quantize_vision_model(model, qconfig_dict):\n",
    "        _, data = cifar_dataloader()\n",
    "        mp = prepare_fx(model, qconfig_dict)\n",
    "\n",
    "        for c, (x, y) in enumerate(data):\n",
    "                if c == 30:\n",
    "                        break\n",
    "                mp(x)\n",
    "        \n",
    "        mc = convert_fx(mp)\n",
    "        return mc\n",
    "\n",
    "static_resnet = static_quantize_vision_model(resnet, qconfig_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance of static-quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resnet Static-Quant Profile:\")\n",
    "profile(static_resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity Analysis - Which quantized layers affect accuracy the most?\n",
    "\n",
    "<img src=\"./img/flowchart-check8.png\" width=\"300\" />\n",
    "<br>\n",
    "\n",
    "Some layers are more sensitive to precision drops than others. PyTorch provides tools to help with this analysis under the Numeric Suite.\n",
    "\n",
    "[Numeric Suite Tutorial](https://pytorch.org/tutorials/prototype/numeric_suite_tutorial.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization._numeric_suite as ns\n",
    "\n",
    "def SNR(x, y):\n",
    "    # Higher is better\n",
    "    Ps = torch.norm(x)\n",
    "    Pn = torch.norm(x-y)\n",
    "    return 20 * torch.log10(Ps/Pn)\n",
    "\n",
    "def compare_model_weights(float_model, quant_model):\n",
    "    snr_dict = {}\n",
    "    wt_compare_dict = ns.compare_weights(float_model.state_dict(), quant_model.state_dict())\n",
    "    for param_name, weight in wt_compare_dict.items():\n",
    "        snr = SNR(weight['float'], weight['quantized'].dequantize())\n",
    "        snr_dict[param_name] = snr\n",
    "\n",
    "    return snr_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer-by-layer comparison of model weights \n",
    "\n",
    "<img src=\"./img/ns.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snrd = compare_model_weights(resnet, static_resnet)\n",
    "print(snrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_sensitive_layers(snr_dict, k):\n",
    "    snr_dict = dict(sorted(snr_dict.items(), key=lambda x:x[1]))\n",
    "    snr_dict = {k.replace('.weight', ''):v for k,v in list(snr_dict.items())[:k]}\n",
    "    return snr_dict\n",
    "    \n",
    "sensitive_layers = topk_sensitive_layers(snrd, 5).keys()\n",
    "print(sensitive_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selective Static Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_layers = topk_sensitive_layers(snrd, 5).keys()\n",
    "\n",
    "qconfig_dict = {\n",
    "    # Global Config\n",
    "    \"\": static_qconfig,\n",
    "\n",
    "    # Disable for sensitive modules\n",
    "    \"module_name\": [(m, None) for m in sensitive_layers],\n",
    "}\n",
    "\n",
    "sel_static_resnet = static_quantize_vision_model(resnet, qconfig_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance of selective static-quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resnet Selective-Static-Quant Profile:\")\n",
    "profile(sel_static_resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization-Aware Training\n",
    "\n",
    "<img src=\"./img/flowchart-check9.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n"
     ]
    }
   ],
   "source": [
    "from torch.quantization.quantize_fx import prepare_qat_fx\n",
    "from resnet_cifar import Trainer, cifar_dataloader\n",
    "\n",
    "sensitive_layers = topk_sensitive_layers(snrd, 5).keys()\n",
    "\n",
    "qat_qconfig = torch.quantization.get_default_qat_qconfig(backend)\n",
    "qconfig_dict = {\n",
    "    # Global Config\n",
    "    \"\": qat_qconfig,\n",
    "}\n",
    "\n",
    "def qat_vision_model(model, qconfig):\n",
    "    model.train()\n",
    "    mp = prepare_qat_fx(model, qconfig)\n",
    "\n",
    "    # training loop\n",
    "    trainer = Trainer(mp, epochs=20)  \n",
    "    trainer.run_epoch()\n",
    "\n",
    "    mc = convert_fx(mp)\n",
    "    return mc\n",
    "\n",
    "qat_resnet = qat_vision_model(resnet, qconfig_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance of QAT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnet QAT Profile:\n",
      "Size (MB): 11.310369\n",
      "====================\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Loss: 0.4549180895090103 \n",
      "Accuracy: 0.8427083333333333\n",
      "====================\n",
      "Time taken (1920 CIFAR test samples): 0.3876371383666992\n"
     ]
    }
   ],
   "source": [
    "print(\"Resnet QAT Profile:\")\n",
    "profile(qat_resnet)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b2c14c5f2a3b21e6c2412c8196f5145870350e81c0b737cae3e5c60eb1e1eac"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch_p38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
