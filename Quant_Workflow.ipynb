{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms, datasets\n",
    "from copy import deepcopy\n",
    "import requests\n",
    "from PIL import Image\n",
    "from resnet_cifar import Trainer, cifar_dataloader\n",
    "\n",
    "\n",
    "def load_img(url):\n",
    "    IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)\n",
    "        ])\n",
    "    if url.startswith(\"https\"):\n",
    "        img = Image.open(requests.get(url, stream=True).raw)\n",
    "    else:\n",
    "        img = Image.open(url)\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_predictions(outp):\n",
    "    cls_idx = {\n",
    "        0: 'airplane',\n",
    "        1: 'automobile',\n",
    "        2: 'bird',\n",
    "        3: 'cat',\n",
    "        4: 'deer',\n",
    "        5: 'dog',\n",
    "        6: 'frog',\n",
    "        7: 'horse',\n",
    "        8: 'ship',\n",
    "        9: 'truck'}\n",
    "    outp = F.softmax(outp, dim=1)\n",
    "    score, idx = torch.topk(outp, 1)\n",
    "    idx.squeeze_()\n",
    "    predicted_label = cls_idx[idx.item()]\n",
    "    print(predicted_label, '(', score.squeeze().item(), ')')\n",
    "\n",
    "\n",
    "def print_sizeof(model):\n",
    "    total = 0\n",
    "    for p in model.parameters():\n",
    "        total += p.numel() * p.element_size()\n",
    "    total /= 1e6\n",
    "    print(\"Model size: \", total, \" MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flowchart for using Quantization in PyTorch\n",
    "\n",
    "<img src=\"./img/quantization-flowchart.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10M+ Parameters?\n",
    "\n",
    "<img src=\"./img/flowchart-check1.png\" width=\"300\" />\n",
    "\n",
    "Quantization works best on models with 10M+ parameters. [[1](https://arxiv.org/pdf/1806.08342.pdf)]\n",
    "\n",
    "Large models are more robust to quantization error. Overparameterized models generally have more degrees of freedom and can afford the precision drops with quantization.\n",
    "\n",
    "As with most thumb rules, YMMV. Quantization is an active area of research, and this might become more permissive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet18:  (True, 11.0)\n",
      "resnet50:  (True, 25.0)\n",
      "mobilenet_large:  (False, 5.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def is_large_enough(model):\n",
    "    n_params = sum([p.numel() for p in model.parameters()])\n",
    "    return n_params > 1e7, n_params // 1e6\n",
    "\n",
    "print(\"resnet18: \", is_large_enough(models.resnet18()))\n",
    "print(\"resnet50: \", is_large_enough(models.resnet50()))\n",
    "print(\"mobilenet_large: \", is_large_enough(models.mobilenet_v3_large()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP32-pretrained checkpoint?\n",
    "\n",
    "<img src=\"./img/flowchart-check2.png\" width=\"300\" />\n",
    "\n",
    "Quantized inference works best on models that were originally trained in FP32 (like all non-quantized pretrained models in PyTorch (vision, audio and text)). \n",
    "Even Quantization-Aware Training (more on this below) uses FP32 arithmetic to train the parameters.\n",
    "\n",
    "....\n",
    "\n",
    "In this exercise, we'll use an FP32 Imagenet-pretrained Resnet that is finetuned to CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://quantization-workshop.s3.amazonaws.com/resnet50_cifar_weights.pth\" to /Users/subramen/.cache/torch/hub/checkpoints/resnet50_cifar_weights.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0663437d03014d39969105b63927093e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/90.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model precision:  torch.float32\n"
     ]
    }
   ],
   "source": [
    "# load model from checkpoint \n",
    "\n",
    "weights = torch.hub.load_state_dict_from_url(\"https://quantization-workshop.s3.amazonaws.com/resnet50_cifar_weights.pth\", map_location=\"cpu\")\n",
    "resnet = models.resnet50(pretrained=False, num_classes=10)\n",
    "# weights = torch.hub.load_state_dict_from_url(\"https://quantization-workshop.s3.amazonaws.com/resnet18_cifar_weights.pth\", map_location=\"cpu\")\n",
    "# resnet = models.resnet18(pretrained=False, num_classes=10)\n",
    "\n",
    "resnet.load_state_dict(weights)\n",
    "\n",
    "param_0 = next(iter(resnet.parameters()))\n",
    "print(\"Model precision: \", param_0.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start quantizing the model, we need to switch it to `eval` mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a supported backend?\n",
    "\n",
    "<img src=\"./img/flowchart-check3.png\" width=\"300\" />\n",
    "\n",
    "Backend refers to the hardware-specific kernels that support quantization. This controls the numerics engine that does the integer arithmetic.\n",
    "\n",
    "`torch.backends.quantized.engine` specifies the backend to be used.\n",
    "\n",
    "Using an incorrect backend engine for your hardware will result in (much) slower inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using qnnpack backend engine for arm CPU\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "chip = platform.processor()\n",
    "\n",
    "if chip == 'arm':\n",
    "    backend = 'qnnpack'\n",
    "elif chip in ['x86_64', 'i386']:\n",
    "    backend = 'fbgemm'\n",
    "else:\n",
    "    raise SystemError(\"Backend is not supported\")\n",
    "\n",
    "print(f\"Using {backend} backend engine for {chip} CPU\")\n",
    "\n",
    "torch.backends.quantized.engine = backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile FP32 model inference\n",
    "\n",
    "<img src=\"./img/flowchart-check4.png\" width=\"300\" />\n",
    "\n",
    "Let's establish a baseline for model size, inference latency and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "    \n",
    "def print_size_of_model(model):\n",
    "    torch.jit.script(model).save(\"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "def profile(model):\n",
    "    print_size_of_model(model)\n",
    "    print(\"=\"*20)\n",
    "    Trainer(model, -1).evaluate(max_batch=30)  # latency + accuracy on CIFAR test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnet FP32 profile:\n",
      "Size (MB): 94.469753\n",
      "====================\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar_data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c1ccc63a89439796d464f229d974e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./cifar_data/cifar-10-python.tar.gz to ./cifar_data\n",
      "Files already downloaded and verified\n",
      "Loss: 0.5838258673747381 \n",
      "Accuracy: 0.8729166666666667\n",
      "====================\n",
      "Time taken (1920 CIFAR test samples): 82.51376509666443\n"
     ]
    }
   ],
   "source": [
    "print(\"Resnet FP32 profile:\")\n",
    "profile(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the predominant layers in the model?\n",
    "\n",
    "<img src=\"./img/flowchart-check5.png\" width=\"300\" />\n",
    "\n",
    "While dynamic quantization has more overhead than static quantization, some operators (like recurrent layers) aren't supported by static quantization. (See [Operator coverage](https://pytorch.org/docs/stable/quantization.html#:~:text=these%20quantization%20types.-,Operator%20coverage,-varies%20between%20dynamic)).\n",
    "\n",
    "Knowing which layers are in our model can inform our quantization strategy.\n",
    "\n",
    "\n",
    "#### Thumb rule\n",
    "\n",
    "* For recurrent and transformer layers, use Dynamic quantization.\n",
    "* For linear layers, you can use either Dynamic or Static quantization.\n",
    "* For everything else, use Static quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model consists of:  Counter({'Conv2d': 53, 'BatchNorm2d': 53, 'ReLU': 17, 'Bottleneck': 16, 'Sequential': 8, 'ResNet': 1, 'MaxPool2d': 1, 'AdaptiveAvgPool2d': 1, 'Linear': 1})\n",
      "\n",
      "Dynamic quantization\n",
      "====================\n",
      "Layers: 1 || Parameters: 20480\n",
      "\n",
      "Static quantization\n",
      "====================\n",
      "Layers: 54 || Parameters: 2.34754e+07\n"
     ]
    }
   ],
   "source": [
    "def optimal_quant_strategy(model):\n",
    "    from collections import Counter\n",
    "    layer_counts = Counter([type(x).__name__ for x in model.modules()])\n",
    "    print(\"Model consists of: \", layer_counts)\n",
    "    \n",
    "    dyn = [0, 0]\n",
    "    stat = [0, 0]\n",
    "\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, 'weight'):    \n",
    "            name = type(m).__name__\n",
    "            params = m.weight.numel()\n",
    "            if name in ['RNN', 'LSTM', 'GRU', 'LSTMCell', 'RNNCell', 'GRUCell', 'Linear']:\n",
    "                dyn[0] += 1\n",
    "                dyn[1] += params\n",
    "            if 'Conv' in name or name == 'Linear':\n",
    "                stat[0] += 1\n",
    "                stat[1] += params\n",
    "    print()\n",
    "    print(\"Dynamic quantization\")\n",
    "    print(\"====================\")\n",
    "    print(f\"Layers: {dyn[0]} || Parameters: {format(dyn[1], 'g')}\")\n",
    "    print()\n",
    "    print(\"Static quantization\")\n",
    "    print(\"====================\")\n",
    "    print(f\"Layers: {stat[0]} || Parameters: {format(stat[1], 'g')}\")\n",
    "    \n",
    "\n",
    "optimal_quant_strategy(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Dynamic Quantization\n",
    "\n",
    "<img src=\"./img/flowchart-check6.png\" width=\"300\" />\n",
    "\n",
    "[Dynamic Quantization API](https://pytorch.org/docs/stable/generated/torch.quantization.quantize_dynamic.html?highlight=quantize_dynamic#torch.quantization.quantize_dynamic)\n",
    "\n",
    "[Dynamic Quantization Tutorial](https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.9/site-packages/torch/quantization/fx/quantization_patterns.py:616: UserWarning: dtype combination: (torch.float32, torch.qint8, torch.quint8) is not supported by Conv supported dtype combinations are: [(torch.quint8, torch.qint8, None)]\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/lib/python3.9/site-packages/torch/quantization/fx/quantization_patterns.py:484: UserWarning: dtype combination: (torch.float32, torch.qint8, torch.quint8) is not supported by <built-in function add> for is_reference=False. Supported non-reference dtype combinations are: [(torch.qint8, torch.qint8, None), (torch.quint8, torch.qint8, None), (torch.float16, torch.float16, None)] \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "\n",
    "dynamic_qconfig = torch.quantization.default_dynamic_qconfig\n",
    "qconfig_dict = {\n",
    "    # Global Config\n",
    "    \"\": dynamic_qconfig\n",
    "}\n",
    "\n",
    "model_prepared = prepare_fx(resnet, qconfig_dict)\n",
    "dynamic_resnet = convert_fx(model_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance of dynamic-quantized Resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnet Dynamic-Quant Profile:\n",
      "Size (MB): 94.057089\n",
      "====================\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Loss: 0.5846356943249702 \n",
      "Accuracy: 0.8723958333333334\n",
      "====================\n",
      "Time taken (1920 CIFAR test samples): 78.73776507377625\n"
     ]
    }
   ],
   "source": [
    "print(\"Resnet Dynamic-Quant Profile:\")\n",
    "profile(dynamic_resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Static Quantization\n",
    "\n",
    "<img src=\"./img/flowchart-check7_1.png\" width=\"300\" />\n",
    "<br>\n",
    "<img src=\"./img/flowchart-check7_2.png\" width=\"300\" />\n",
    "\n",
    "Static quantization has faster inference than dynamic quantization because it eliminates the float<->int conversion costs between layers\n",
    "\n",
    "### Manual approach - using Eager Mode\n",
    "\n",
    "Explicitly perform the following steps:\n",
    "\n",
    "<img src=\"./img/ptq-flowchart.png\" width=\"300\" />\n",
    "\n",
    "* Manually identify sequence of fusable modules\n",
    "* Manually insert stubs to quantize and dequantize activations\n",
    "* Functional ops (eg: `torch.nn.functional.linear`) aren't supported\n",
    "\n",
    "[Module Fusion Tutorial](https://pytorch.org/tutorials/recipes/fuse.html)\n",
    "\n",
    "[Static Quantization (Eager Mode) Tutorial](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easier approach - using FX Graph Mode\n",
    "\n",
    "<img src=\"./img/ptq-fx-flowchart.png\" width=\"300\" />\n",
    "\n",
    "* Just 2 function calls: `prepare_fx` and `convert_fx`\n",
    "* Automates all the above steps under the hood using `torch.fx`\n",
    "\n",
    "[`prepare_fx` API](https://pytorch.org/docs/stable/generated/torch.quantization.quantize_fx.prepare_fx.html#torch.quantization.quantize_fx.prepare_fx)\n",
    "\n",
    "[`convert_fx` API](https://pytorch.org/docs/stable/generated/torch.quantization.quantize_fx.convert_fx.html#torch.quantization.quantize_fx.convert_fx)\n",
    "\n",
    "[Static Quantization (FX Graph Mode) Tutorial](https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QConfig\n",
    "\n",
    "In FX Quantization, the `qconfig_dict` offers fine-grained control of the model's quantization process.\n",
    "Setting a `qconfig=None` skips quantization for that module.\n",
    "\n",
    "```python\n",
    "qconfig_dict = {\n",
    "    # Global Config\n",
    "    \"\": qconfig,\n",
    "\n",
    "    # Module-specific config (by class)\n",
    "    \"object_type\": [\n",
    "        (torch.nn.Conv2d, qconfig),\n",
    "        (torch.nn.functional.add, None),  # skips quantization for this module\n",
    "        ...,\n",
    "        ],\n",
    "    \n",
    "    # Module-specific config (by name)\n",
    "    \"module_name\": [\n",
    "        (\"foo.bar\", qconfig)\n",
    "        ...,\n",
    "    ],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "qconfig_dict = {\n",
    "    # Global Config\n",
    "    \"\": static_qconfig,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.9/site-packages/torch/ao/quantization/observer.py:886: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  src_bin_begin // dst_bin_width, 0, self.dst_nbins - 1\n",
      "/opt/miniconda3/lib/python3.9/site-packages/torch/ao/quantization/observer.py:891: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  src_bin_end // dst_bin_width, 0, self.dst_nbins - 1\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "\n",
    "\n",
    "def static_quantize_vision_model(model, qconfig_dict):\n",
    "        _, data = cifar_dataloader()\n",
    "        mp = prepare_fx(model, qconfig_dict)\n",
    "\n",
    "        for c, (x, y) in enumerate(data):\n",
    "                if c == 30:\n",
    "                        break\n",
    "                mp(x)\n",
    "        \n",
    "        mc = convert_fx(mp)\n",
    "        return mc\n",
    "\n",
    "static_resnet = static_quantize_vision_model(resnet, qconfig_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance of static-quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnet Static-Quant Profile:\n",
      "Size (MB): 23.661729\n",
      "====================\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Loss: 0.5700115218758584 \n",
      "Accuracy: 0.8708333333333333\n",
      "====================\n",
      "Time taken (1920 CIFAR test samples): 34.77193212509155\n"
     ]
    }
   ],
   "source": [
    "print(\"Resnet Static-Quant Profile:\")\n",
    "profile(static_resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity Analysis - Which quantized layers affect accuracy the most?\n",
    "\n",
    "<img src=\"./img/flowchart-check8.png\" width=\"300\" />\n",
    "<br>\n",
    "\n",
    "Some layers are more sensitive to precision drops than others. PyTorch provides tools to help with this analysis under the Numeric Suite.\n",
    "\n",
    "[Numeric Suite Tutorial](https://pytorch.org/tutorials/prototype/numeric_suite_tutorial.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization._numeric_suite as ns\n",
    "\n",
    "def SNR(x, y):\n",
    "    # Higher is better\n",
    "    Ps = torch.norm(x)\n",
    "    Pn = torch.norm(x-y)\n",
    "    return 20 * torch.log10(Ps/Pn)\n",
    "\n",
    "def compare_model_weights(float_model, quant_model):\n",
    "    snr_dict = {}\n",
    "    wt_compare_dict = ns.compare_weights(float_model.state_dict(), quant_model.state_dict())\n",
    "    for param_name, weight in wt_compare_dict.items():\n",
    "        snr = SNR(weight['float'], weight['quantized'].dequantize())\n",
    "        snr_dict[param_name] = snr\n",
    "\n",
    "    return snr_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer-by-layer comparison of model weights \n",
    "\n",
    "<img src=\"./img/ns.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conv1.weight': tensor(0.1324), 'layer1.0.conv1.weight': tensor(1.6452), 'layer1.0.conv2.weight': tensor(-2.4947), 'layer1.0.conv3.weight': tensor(-10.0644), 'layer1.0.downsample.0.weight': tensor(1.3075), 'layer1.1.conv1.weight': tensor(3.2131), 'layer1.1.conv2.weight': tensor(1.2505), 'layer1.1.conv3.weight': tensor(-9.6478), 'layer1.2.conv1.weight': tensor(-0.2502), 'layer1.2.conv2.weight': tensor(-6.6366), 'layer1.2.conv3.weight': tensor(-12.0677), 'layer2.0.conv1.weight': tensor(-0.1468), 'layer2.0.conv2.weight': tensor(1.0737), 'layer2.0.conv3.weight': tensor(-9.7258), 'layer2.0.downsample.0.weight': tensor(-0.3786), 'layer2.1.conv1.weight': tensor(6.1034), 'layer2.1.conv2.weight': tensor(-3.3722), 'layer2.1.conv3.weight': tensor(-13.6104), 'layer2.2.conv1.weight': tensor(5.4238), 'layer2.2.conv2.weight': tensor(-5.2156), 'layer2.2.conv3.weight': tensor(-13.1915), 'layer2.3.conv1.weight': tensor(3.8682), 'layer2.3.conv2.weight': tensor(-1.7579), 'layer2.3.conv3.weight': tensor(-10.4095), 'layer3.0.conv1.weight': tensor(0.8389), 'layer3.0.conv2.weight': tensor(2.5050), 'layer3.0.conv3.weight': tensor(-8.3334), 'layer3.0.downsample.0.weight': tensor(1.4675), 'layer3.1.conv1.weight': tensor(-3.4588), 'layer3.1.conv2.weight': tensor(-10.1038), 'layer3.1.conv3.weight': tensor(-14.9413), 'layer3.2.conv1.weight': tensor(-5.7936), 'layer3.2.conv2.weight': tensor(-13.7164), 'layer3.2.conv3.weight': tensor(-15.2606), 'layer3.3.conv1.weight': tensor(-6.9320), 'layer3.3.conv2.weight': tensor(-14.8504), 'layer3.3.conv3.weight': tensor(-15.1983), 'layer3.4.conv1.weight': tensor(-7.3590), 'layer3.4.conv2.weight': tensor(-11.8682), 'layer3.4.conv3.weight': tensor(-13.7047), 'layer3.5.conv1.weight': tensor(-6.7509), 'layer3.5.conv2.weight': tensor(-13.1252), 'layer3.5.conv3.weight': tensor(-15.3381), 'layer4.0.conv1.weight': tensor(-5.8595), 'layer4.0.conv2.weight': tensor(-10.9096), 'layer4.0.conv3.weight': tensor(-19.0842), 'layer4.0.downsample.0.weight': tensor(-15.0892), 'layer4.1.conv1.weight': tensor(5.9120), 'layer4.1.conv2.weight': tensor(-17.6098), 'layer4.1.conv3.weight': tensor(-22.6924), 'layer4.2.conv1.weight': tensor(7.8386), 'layer4.2.conv2.weight': tensor(-17.1798), 'layer4.2.conv3.weight': tensor(-28.5748), 'fc._packed_params._packed_params': tensor(29.6848)}\n"
     ]
    }
   ],
   "source": [
    "snrd = compare_model_weights(resnet, static_resnet)\n",
    "print(snrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['layer4.2.conv3', 'layer4.1.conv3', 'layer4.0.conv3', 'layer4.1.conv2', 'layer4.2.conv2'])\n"
     ]
    }
   ],
   "source": [
    "def topk_sensitive_layers(snr_dict, k):\n",
    "    snr_dict = dict(sorted(snr_dict.items(), key=lambda x:x[1]))\n",
    "    snr_dict = {k.replace('.weight', ''):v for k,v in list(snr_dict.items())[:k]}\n",
    "    return snr_dict\n",
    "    \n",
    "sensitive_layers = topk_sensitive_layers(snrd, 5).keys()\n",
    "print(sensitive_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selective Static Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "sensitive_layers = topk_sensitive_layers(snrd, 5).keys()\n",
    "\n",
    "qconfig_dict = {\n",
    "    # Global Config\n",
    "    \"\": static_qconfig,\n",
    "\n",
    "    # Disable for sensitive modules\n",
    "    \"module_name\": [(m, None) for m in sensitive_layers],\n",
    "}\n",
    "\n",
    "sel_static_resnet = static_quantize_vision_model(resnet, qconfig_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance of selective static-quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnet Selective-Static-Quant Profile:\n",
      "Size (MB): 47.265735\n",
      "====================\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Loss: 0.5818707610170046 \n",
      "Accuracy: 0.8703125\n",
      "====================\n",
      "Time taken (1920 CIFAR test samples): 42.71668791770935\n"
     ]
    }
   ],
   "source": [
    "print(\"Resnet Selective-Static-Quant Profile:\")\n",
    "profile(sel_static_resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization-Aware Training\n",
    "\n",
    "<img src=\"./img/flowchart-check9.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization.quantize_fx import prepare_qat_fx\n",
    "from resnet_cifar import Trainer, cifar_dataloader\n",
    "\n",
    "sensitive_layers = topk_sensitive_layers(snrd, 5).keys()\n",
    "\n",
    "qat_qconfig = torch.quantization.get_default_qat_qconfig(backend)\n",
    "qconfig_dict = {\n",
    "    # Global Config\n",
    "    \"\": qat_qconfig,\n",
    "}\n",
    "\n",
    "def qat_vision_model(model, qconfig):\n",
    "    model.train()\n",
    "    mp = prepare_qat_fx(model, qconfig)\n",
    "\n",
    "    # training loop\n",
    "    trainer = Trainer(mp, epochs=120, device='cuda')  \n",
    "    trainer.run_epoch()\n",
    "    mp = mp.cpu()\n",
    "\n",
    "    mc = convert_fx(mp)\n",
    "    return mc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training this for 120 epochs takes about 2 hours on a single Tesla V100 GPU.\n",
    "If you don't want to wait, download the QAT weights instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.9/site-packages/torch/serialization.py:602: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# qat_resnet = qat_vision_model(resnet, qconfig_dict) \n",
    "# torch.jit.script(qat_resnet).save('qat_resnet50_cifar.pt')\n",
    "\n",
    "import requests\n",
    "r = requests.get(\"https://quantization-workshop.s3.amazonaws.com/qat_resnet50_cifar.pt\")\n",
    "open('qat_resnet50_cifar.pt', 'wb').write(r.content)\n",
    "\n",
    "qat_resnet = torch.load(\"qat_resnet50_cifar.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance of QAT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnet QAT Profile:\n",
      "Size (MB): 24.105327\n",
      "====================\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Loss: 0.3617867136994998 \n",
      "Accuracy: 0.8880208333333334\n",
      "====================\n",
      "Time taken (1920 CIFAR test samples): 37.88538384437561\n"
     ]
    }
   ],
   "source": [
    "print(\"Resnet QAT Profile:\")\n",
    "profile(qat_resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the QAT model is the highest, even slightly higher than the FP32 one! This is atypical, and is most likely because the CIFAR10 dataset is very simple for the Resnet architecture. Typically, we'd expect accuracies to drop from FP32-levels for more complex jobs."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
